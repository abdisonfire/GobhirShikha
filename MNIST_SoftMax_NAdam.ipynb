{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RFoJDzBXuP4-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations to apply to the data\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Load the MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)"
      ],
      "metadata": {
        "id": "gVKJZwdbzGth"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(4)\n",
        "train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)"
      ],
      "metadata": {
        "id": "JAOK4Bk0z3pk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ShonkhaModel(nn.Module):\n",
        "  def __init__(self, input, hidden_layer1, hidden_layer2, output):\n",
        "        super(ShonkhaModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input, hidden_layer1)\n",
        "        self.fc2 = nn.Linear(hidden_layer1, hidden_layer2)\n",
        "        self.out = nn.Linear(hidden_layer2, output)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = F.softmax(self.fc1(x), dim=1)\n",
        "      x = F.softmax(self.fc2(x), dim=1)\n",
        "      x = F.softmax(self.out(x), dim=1)\n",
        "      return x"
      ],
      "metadata": {
        "id": "4b_3wU543w40"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "shonkhaObject = ShonkhaModel(784, 784//4, 784//8, 10).to(torch.device('cuda'))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.NAdam(shonkhaObject.parameters(), lr=0.01)\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "precisions_micro = []\n",
        "recalls_micro = []\n",
        "f1_scores_micro = []\n",
        "precisions_macro = []\n",
        "recalls_macro = []\n",
        "f1_scores_macro = []\n",
        "precisions_weighted = []\n",
        "recalls_weighted = []\n",
        "f1_scores_weighted = []\n",
        "\n",
        "for i in range(20):\n",
        "  curr_loss = 0.0\n",
        "  y_true = []\n",
        "  y_pred = []\n",
        "\n",
        "  for j, data in enumerate(train_loader):\n",
        "    input, output = data[0].to(torch.device('cuda')), data[1].to(torch.device('cuda'))\n",
        "    input = input.reshape(50, -1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    pred_out = shonkhaObject.forward(input)\n",
        "    loss = criterion(pred_out, output)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    curr_loss += loss.item()\n",
        "    y_true += output.tolist()\n",
        "    y_pred += torch.argmax(pred_out, dim=1).tolist()\n",
        "\n",
        "  loss_avg = curr_loss / len(train_loader)\n",
        "  losses.append(loss_avg)\n",
        "\n",
        "  acc = accuracy_score(y_true, y_pred)\n",
        "  accuracies.append(acc*100)\n",
        "\n",
        "  precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(y_true, y_pred, average='micro', zero_division=1)\n",
        "  precisions_micro.append(precision_micro)\n",
        "  recalls_micro.append(recall_micro)\n",
        "  f1_scores_micro.append(f1_micro)\n",
        "\n",
        "  precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=1)\n",
        "  precisions_macro.append(precision_macro)\n",
        "  recalls_macro.append(recall_macro)\n",
        "  f1_scores_macro.append(f1_macro)\n",
        "\n",
        "  precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=1)\n",
        "  precisions_weighted.append(precision_weighted)\n",
        "  recalls_weighted.append(recall_weighted)\n",
        "  f1_scores_weighted.append(f1_weighted)\n",
        "  print(f'epoch: {i:2}  loss: {loss_avg}')\n",
        "\n",
        "\n",
        "\n",
        "perf_metrics = {\n",
        "    'loss': losses,\n",
        "    'accuracy': accuracies,\n",
        "    'precision_micro': precisions_micro,\n",
        "    'recall_micro': recalls_micro,\n",
        "    'f1_micro': f1_scores_micro,\n",
        "    'precision_macro': precisions_macro,\n",
        "    'recall_macro': recalls_macro,\n",
        "    'f1_macro': f1_scores_macro,\n",
        "    'precision_weighted': precisions_weighted,\n",
        "    'recall_weighted': recalls_weighted,\n",
        "    'f1_weighted': f1_scores_weighted\n",
        "}\n",
        "\n",
        "perf_metrics_df = pd.DataFrame(perf_metrics)\n",
        "display(perf_metrics_df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q7lxf57_1wyG",
        "outputId": "bd08f67d-da84-4bc5-e8fc-81ac36a798fc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0  loss: 1.7896734617153804\n",
            "epoch:  1  loss: 1.5836117085814476\n",
            "epoch:  2  loss: 1.565218727986018\n",
            "epoch:  3  loss: 1.5577450918157896\n",
            "epoch:  4  loss: 1.5581305694580079\n",
            "epoch:  5  loss: 1.5554171317815781\n",
            "epoch:  6  loss: 1.5536490434408188\n",
            "epoch:  7  loss: 1.551723353266716\n",
            "epoch:  8  loss: 1.549396420419216\n",
            "epoch:  9  loss: 1.5466203985611597\n",
            "epoch: 10  loss: 1.5522823895017306\n",
            "epoch: 11  loss: 1.5488002429405847\n",
            "epoch: 12  loss: 1.5473155574997266\n",
            "epoch: 13  loss: 1.5454508244991303\n",
            "epoch: 14  loss: 1.542898369928201\n",
            "epoch: 15  loss: 1.546383292078972\n",
            "epoch: 16  loss: 1.544390830496947\n",
            "epoch: 17  loss: 1.5454422925909361\n",
            "epoch: 18  loss: 1.5432121246059736\n",
            "epoch: 19  loss: 1.545466067393621\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        loss   accuracy  precision_micro  recall_micro  f1_micro  \\\n",
              "0   1.789673  70.013333         0.700133      0.700133  0.700133   \n",
              "1   1.583612  87.985000         0.879850      0.879850  0.879850   \n",
              "2   1.565219  89.635000         0.896350      0.896350  0.896350   \n",
              "3   1.557745  90.358333         0.903583      0.903583  0.903583   \n",
              "4   1.558131  90.308333         0.903083      0.903083  0.903083   \n",
              "5   1.555417  90.563333         0.905633      0.905633  0.905633   \n",
              "6   1.553649  90.735000         0.907350      0.907350  0.907350   \n",
              "7   1.551723  90.938333         0.909383      0.909383  0.909383   \n",
              "8   1.549396  91.173333         0.911733      0.911733  0.911733   \n",
              "9   1.546620  91.451667         0.914517      0.914517  0.914517   \n",
              "10  1.552282  90.875000         0.908750      0.908750  0.908750   \n",
              "11  1.548800  91.236667         0.912367      0.912367  0.912367   \n",
              "12  1.547316  91.375000         0.913750      0.913750  0.913750   \n",
              "13  1.545451  91.563333         0.915633      0.915633  0.915633   \n",
              "14  1.542898  91.821667         0.918217      0.918217  0.918217   \n",
              "15  1.546383  91.471667         0.914717      0.914717  0.914717   \n",
              "16  1.544391  91.680000         0.916800      0.916800  0.916800   \n",
              "17  1.545442  91.570000         0.915700      0.915700  0.915700   \n",
              "18  1.543212  91.788333         0.917883      0.917883  0.917883   \n",
              "19  1.545466  91.563333         0.915633      0.915633  0.915633   \n",
              "\n",
              "    precision_macro  recall_macro  f1_macro  precision_weighted  \\\n",
              "0          0.743578      0.689683  0.648100            0.740757   \n",
              "1          0.877573      0.878003  0.877518            0.879115   \n",
              "2          0.895229      0.895247  0.895167            0.896405   \n",
              "3          0.902867      0.902856  0.902694            0.904019   \n",
              "4          0.902497      0.902486  0.902134            0.903483   \n",
              "5          0.904959      0.904908  0.904573            0.905981   \n",
              "6          0.906764      0.906730  0.906309            0.907708   \n",
              "7          0.908641      0.908601  0.908472            0.909335   \n",
              "8          0.911190      0.910946  0.910790            0.911964   \n",
              "9          0.913916      0.913735  0.913758            0.914564   \n",
              "10         0.908221      0.907779  0.907755            0.908574   \n",
              "11         0.911770      0.911780  0.911628            0.912647   \n",
              "12         0.913220      0.913011  0.913014            0.913984   \n",
              "13         0.915300      0.914869  0.914986            0.916063   \n",
              "14         0.918023      0.917474  0.917551            0.918688   \n",
              "15         0.914293      0.913813  0.913976            0.914574   \n",
              "16         0.916366      0.916211  0.916153            0.917090   \n",
              "17         0.915100      0.915091  0.914982            0.915900   \n",
              "18         0.917842      0.917269  0.917421            0.918275   \n",
              "19         0.915101      0.914829  0.914873            0.915729   \n",
              "\n",
              "    recall_weighted  f1_weighted  \n",
              "0          0.700133     0.655398  \n",
              "1          0.879850     0.879216  \n",
              "2          0.896350     0.896307  \n",
              "3          0.903583     0.903628  \n",
              "4          0.903083     0.902919  \n",
              "5          0.905633     0.905452  \n",
              "6          0.907350     0.907096  \n",
              "7          0.909383     0.909212  \n",
              "8          0.911733     0.911572  \n",
              "9          0.914517     0.914474  \n",
              "10         0.908750     0.908419  \n",
              "11         0.912367     0.912357  \n",
              "12         0.913750     0.913768  \n",
              "13         0.915633     0.915748  \n",
              "14         0.918217     0.918260  \n",
              "15         0.914717     0.914571  \n",
              "16         0.916800     0.916805  \n",
              "17         0.915700     0.915687  \n",
              "18         0.917883     0.917948  \n",
              "19         0.915633     0.915591  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-86394b6f-238a-4d3e-ad63-48ce8bcfba82\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_micro</th>\n",
              "      <th>recall_micro</th>\n",
              "      <th>f1_micro</th>\n",
              "      <th>precision_macro</th>\n",
              "      <th>recall_macro</th>\n",
              "      <th>f1_macro</th>\n",
              "      <th>precision_weighted</th>\n",
              "      <th>recall_weighted</th>\n",
              "      <th>f1_weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.789673</td>\n",
              "      <td>70.013333</td>\n",
              "      <td>0.700133</td>\n",
              "      <td>0.700133</td>\n",
              "      <td>0.700133</td>\n",
              "      <td>0.743578</td>\n",
              "      <td>0.689683</td>\n",
              "      <td>0.648100</td>\n",
              "      <td>0.740757</td>\n",
              "      <td>0.700133</td>\n",
              "      <td>0.655398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.583612</td>\n",
              "      <td>87.985000</td>\n",
              "      <td>0.879850</td>\n",
              "      <td>0.879850</td>\n",
              "      <td>0.879850</td>\n",
              "      <td>0.877573</td>\n",
              "      <td>0.878003</td>\n",
              "      <td>0.877518</td>\n",
              "      <td>0.879115</td>\n",
              "      <td>0.879850</td>\n",
              "      <td>0.879216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.565219</td>\n",
              "      <td>89.635000</td>\n",
              "      <td>0.896350</td>\n",
              "      <td>0.896350</td>\n",
              "      <td>0.896350</td>\n",
              "      <td>0.895229</td>\n",
              "      <td>0.895247</td>\n",
              "      <td>0.895167</td>\n",
              "      <td>0.896405</td>\n",
              "      <td>0.896350</td>\n",
              "      <td>0.896307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.557745</td>\n",
              "      <td>90.358333</td>\n",
              "      <td>0.903583</td>\n",
              "      <td>0.903583</td>\n",
              "      <td>0.903583</td>\n",
              "      <td>0.902867</td>\n",
              "      <td>0.902856</td>\n",
              "      <td>0.902694</td>\n",
              "      <td>0.904019</td>\n",
              "      <td>0.903583</td>\n",
              "      <td>0.903628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.558131</td>\n",
              "      <td>90.308333</td>\n",
              "      <td>0.903083</td>\n",
              "      <td>0.903083</td>\n",
              "      <td>0.903083</td>\n",
              "      <td>0.902497</td>\n",
              "      <td>0.902486</td>\n",
              "      <td>0.902134</td>\n",
              "      <td>0.903483</td>\n",
              "      <td>0.903083</td>\n",
              "      <td>0.902919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.555417</td>\n",
              "      <td>90.563333</td>\n",
              "      <td>0.905633</td>\n",
              "      <td>0.905633</td>\n",
              "      <td>0.905633</td>\n",
              "      <td>0.904959</td>\n",
              "      <td>0.904908</td>\n",
              "      <td>0.904573</td>\n",
              "      <td>0.905981</td>\n",
              "      <td>0.905633</td>\n",
              "      <td>0.905452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.553649</td>\n",
              "      <td>90.735000</td>\n",
              "      <td>0.907350</td>\n",
              "      <td>0.907350</td>\n",
              "      <td>0.907350</td>\n",
              "      <td>0.906764</td>\n",
              "      <td>0.906730</td>\n",
              "      <td>0.906309</td>\n",
              "      <td>0.907708</td>\n",
              "      <td>0.907350</td>\n",
              "      <td>0.907096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.551723</td>\n",
              "      <td>90.938333</td>\n",
              "      <td>0.909383</td>\n",
              "      <td>0.909383</td>\n",
              "      <td>0.909383</td>\n",
              "      <td>0.908641</td>\n",
              "      <td>0.908601</td>\n",
              "      <td>0.908472</td>\n",
              "      <td>0.909335</td>\n",
              "      <td>0.909383</td>\n",
              "      <td>0.909212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.549396</td>\n",
              "      <td>91.173333</td>\n",
              "      <td>0.911733</td>\n",
              "      <td>0.911733</td>\n",
              "      <td>0.911733</td>\n",
              "      <td>0.911190</td>\n",
              "      <td>0.910946</td>\n",
              "      <td>0.910790</td>\n",
              "      <td>0.911964</td>\n",
              "      <td>0.911733</td>\n",
              "      <td>0.911572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.546620</td>\n",
              "      <td>91.451667</td>\n",
              "      <td>0.914517</td>\n",
              "      <td>0.914517</td>\n",
              "      <td>0.914517</td>\n",
              "      <td>0.913916</td>\n",
              "      <td>0.913735</td>\n",
              "      <td>0.913758</td>\n",
              "      <td>0.914564</td>\n",
              "      <td>0.914517</td>\n",
              "      <td>0.914474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.552282</td>\n",
              "      <td>90.875000</td>\n",
              "      <td>0.908750</td>\n",
              "      <td>0.908750</td>\n",
              "      <td>0.908750</td>\n",
              "      <td>0.908221</td>\n",
              "      <td>0.907779</td>\n",
              "      <td>0.907755</td>\n",
              "      <td>0.908574</td>\n",
              "      <td>0.908750</td>\n",
              "      <td>0.908419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1.548800</td>\n",
              "      <td>91.236667</td>\n",
              "      <td>0.912367</td>\n",
              "      <td>0.912367</td>\n",
              "      <td>0.912367</td>\n",
              "      <td>0.911770</td>\n",
              "      <td>0.911780</td>\n",
              "      <td>0.911628</td>\n",
              "      <td>0.912647</td>\n",
              "      <td>0.912367</td>\n",
              "      <td>0.912357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1.547316</td>\n",
              "      <td>91.375000</td>\n",
              "      <td>0.913750</td>\n",
              "      <td>0.913750</td>\n",
              "      <td>0.913750</td>\n",
              "      <td>0.913220</td>\n",
              "      <td>0.913011</td>\n",
              "      <td>0.913014</td>\n",
              "      <td>0.913984</td>\n",
              "      <td>0.913750</td>\n",
              "      <td>0.913768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1.545451</td>\n",
              "      <td>91.563333</td>\n",
              "      <td>0.915633</td>\n",
              "      <td>0.915633</td>\n",
              "      <td>0.915633</td>\n",
              "      <td>0.915300</td>\n",
              "      <td>0.914869</td>\n",
              "      <td>0.914986</td>\n",
              "      <td>0.916063</td>\n",
              "      <td>0.915633</td>\n",
              "      <td>0.915748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1.542898</td>\n",
              "      <td>91.821667</td>\n",
              "      <td>0.918217</td>\n",
              "      <td>0.918217</td>\n",
              "      <td>0.918217</td>\n",
              "      <td>0.918023</td>\n",
              "      <td>0.917474</td>\n",
              "      <td>0.917551</td>\n",
              "      <td>0.918688</td>\n",
              "      <td>0.918217</td>\n",
              "      <td>0.918260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1.546383</td>\n",
              "      <td>91.471667</td>\n",
              "      <td>0.914717</td>\n",
              "      <td>0.914717</td>\n",
              "      <td>0.914717</td>\n",
              "      <td>0.914293</td>\n",
              "      <td>0.913813</td>\n",
              "      <td>0.913976</td>\n",
              "      <td>0.914574</td>\n",
              "      <td>0.914717</td>\n",
              "      <td>0.914571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1.544391</td>\n",
              "      <td>91.680000</td>\n",
              "      <td>0.916800</td>\n",
              "      <td>0.916800</td>\n",
              "      <td>0.916800</td>\n",
              "      <td>0.916366</td>\n",
              "      <td>0.916211</td>\n",
              "      <td>0.916153</td>\n",
              "      <td>0.917090</td>\n",
              "      <td>0.916800</td>\n",
              "      <td>0.916805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1.545442</td>\n",
              "      <td>91.570000</td>\n",
              "      <td>0.915700</td>\n",
              "      <td>0.915700</td>\n",
              "      <td>0.915700</td>\n",
              "      <td>0.915100</td>\n",
              "      <td>0.915091</td>\n",
              "      <td>0.914982</td>\n",
              "      <td>0.915900</td>\n",
              "      <td>0.915700</td>\n",
              "      <td>0.915687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1.543212</td>\n",
              "      <td>91.788333</td>\n",
              "      <td>0.917883</td>\n",
              "      <td>0.917883</td>\n",
              "      <td>0.917883</td>\n",
              "      <td>0.917842</td>\n",
              "      <td>0.917269</td>\n",
              "      <td>0.917421</td>\n",
              "      <td>0.918275</td>\n",
              "      <td>0.917883</td>\n",
              "      <td>0.917948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1.545466</td>\n",
              "      <td>91.563333</td>\n",
              "      <td>0.915633</td>\n",
              "      <td>0.915633</td>\n",
              "      <td>0.915633</td>\n",
              "      <td>0.915101</td>\n",
              "      <td>0.914829</td>\n",
              "      <td>0.914873</td>\n",
              "      <td>0.915729</td>\n",
              "      <td>0.915633</td>\n",
              "      <td>0.915591</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86394b6f-238a-4d3e-ad63-48ce8bcfba82')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-86394b6f-238a-4d3e-ad63-48ce8bcfba82 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-86394b6f-238a-4d3e-ad63-48ce8bcfba82');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(20), losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "vE_k4j4O3BbN",
        "outputId": "692a9435-2e7c-46f9-94e2-e994154843ef"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fe9a418ac70>]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "target_out = []\n",
        "pred_out = []\n",
        "with torch.no_grad():\n",
        "    for (i, data) in enumerate(test_loader):\n",
        "        input, output = data[0].to(torch.device('cuda')), data[1].to(torch.device('cuda'))\n",
        "        # Predict the class of the image\n",
        "        input = input.reshape(100, -1)\n",
        "        outputs = shonkhaObject.forward(input)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        pred_out += predicted.tolist()\n",
        "        target_out += output.tolist()\n",
        "\n",
        "\n",
        "loss_avg = curr_loss / len(train_loader)\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(y_true, y_pred, average='micro', zero_division=1)\n",
        "\n",
        "\n",
        "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=1)\n",
        "\n",
        "\n",
        "precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "\n",
        "perf_metrics = {\n",
        "    'loss': [loss_avg],\n",
        "    'accuracy': [acc*100],\n",
        "    'precision_micro': [precision_micro*100],\n",
        "    'recall_micro': [recall_micro*100],\n",
        "    'f1_micro': [f1_micro*100],\n",
        "    'precision_macro': [precision_macro*100],\n",
        "    'recall_macro': [recall_macro*100],\n",
        "    'f1_macro': [f1_macro*100],\n",
        "    'precision_weighted': [precision_weighted*100],\n",
        "    'recall_weighted': [recall_weighted*100],\n",
        "    'f1_weighted': [f1_weighted*100]\n",
        "}\n",
        "\n",
        "perf_metrics_df = pd.DataFrame(perf_metrics)\n",
        "display(perf_metrics_df)\n"
      ],
      "metadata": {
        "id": "1tMryjsyQWE2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "eed33e8c-830a-4903-e69f-237c2cecf223"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       loss   accuracy  precision_micro  recall_micro   f1_micro  \\\n",
              "0  1.545466  91.563333        91.563333     91.563333  91.563333   \n",
              "\n",
              "   precision_macro  recall_macro   f1_macro  precision_weighted  \\\n",
              "0        91.510056     91.482935  91.487287           91.572878   \n",
              "\n",
              "   recall_weighted  f1_weighted  \n",
              "0        91.563333    91.559094  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e3354e25-6745-4f18-b0bc-2221b2ef1a15\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_micro</th>\n",
              "      <th>recall_micro</th>\n",
              "      <th>f1_micro</th>\n",
              "      <th>precision_macro</th>\n",
              "      <th>recall_macro</th>\n",
              "      <th>f1_macro</th>\n",
              "      <th>precision_weighted</th>\n",
              "      <th>recall_weighted</th>\n",
              "      <th>f1_weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.545466</td>\n",
              "      <td>91.563333</td>\n",
              "      <td>91.563333</td>\n",
              "      <td>91.563333</td>\n",
              "      <td>91.563333</td>\n",
              "      <td>91.510056</td>\n",
              "      <td>91.482935</td>\n",
              "      <td>91.487287</td>\n",
              "      <td>91.572878</td>\n",
              "      <td>91.563333</td>\n",
              "      <td>91.559094</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3354e25-6745-4f18-b0bc-2221b2ef1a15')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e3354e25-6745-4f18-b0bc-2221b2ef1a15 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e3354e25-6745-4f18-b0bc-2221b2ef1a15');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}